function behr_generate_uncertainty_files(varargin)
%BEHR_GENERATE_UNCERTAINTY_FILES Generate the intermediate files for BEHR uncertainty analysis
%   BEHR_GENERATURE_UNCERTAINTY_FILES(  ) Generates files containing Delta
%   and DeltaGrid structures, which are generated by
%   BEHR_UNCERTAINTY_ESTIMATION() and contain the percent change in NO2
%   VCDs and AMFs for a given variation of input parameters. This function
%   will iterate over MODISAlbedo, GLOBETerpres, CloudPressure,
%   CloudRadianceFraction, and the NO2 profiles and generate uncertainty
%   estimation files for each. By default it generates them for March,
%   June, September, and December of 2012 and stores the files under
%   "us-uncertainty" in the behr_paths.behr_mat_dir folder. The following
%   parameters allow you to override those options:
%
%       'test_year' - default is 2012, can be set to any year with BEHR
%       data, though daily profiles are necessary for one of the tests.
%       Give the year as a number.
%
%       'test_months' - a vector of months, given as numbers 1-12,
%       specifying which months of the year to test. 
%
%       'output_root' - the root directory where each input parameter
%       tested will have its subdirectory placed. Default is
%       fullfile(behr_paths.behr_mat_dir, 'us-uncertainty').
%
%       'region' - which region to test. Default is 'us'.
%
%       'prof_mode' - which BEHR subproduct to use. Default is 'daily'.
%       Currently, 'monthly' will fail because the profile time uncertainty
%       test relies on daily profiles.
%
%       'overwrite' - whether output files should be overwritten, or a day
%       skipped if it already exists. Default is false, i.e. do not
%       overwrite.

p = inputParser;
p.addParameter('test_year', 2012);
p.addParameter('test_months', 1:12);
p.addParameter('output_root', '');
p.addParameter('region', 'us');
p.addParameter('prof_mode', 'daily');
p.addParameter('overwrite', false);

p.parse(varargin{:});
pout = p.Results;

test_year = pout.test_year;
test_months = pout.test_months;
prof_mode = pout.prof_mode;
region = pout.region;
output_root = set_output_root(pout.output_root, region);
overwrite = pout.overwrite;

E = JLLErrors;

if ~isnumeric(test_year) || ~isscalar(test_year)
    E.badinput('"test_year" must be a scalar')
end
if ~isnumeric(test_months) || any(test_months(:) < 1 | test_months(:) > 12)
    E.badinput('"test_months" must be an array of numbers between 1 and 12')
end
if ~ischar(prof_mode) || ~strcmpi(prof_mode, 'daily')
    E.badinput('"prof_mode" must be the char ''daily''. Currently, ''monthly'' is not supported')
end
if ~ischar(region)
    % Assume that load_behr_file() will error if an invalid region is given
    E.badinput('"region" must be a char array');
end
if ~islogical(overwrite) || ~isscalar(overwrite)
    E.badinput('"overwrite" must be a scalar logical');
end

% Define the standard parameters to vary here. Give the percent change for
% each month; this allows for the fact that e.g. albedo can be more
% uncertain in winter when there is snow on the ground. "ProfileLoc" and
% "ProfileTime" are special parameter names that tell
% behr_uncertainty_estimation() to have BEHR_main_one_day randomize the
% profile location and day, respectively. The percent difference does not
% matter there because it uses the intrinsic variability of the profiles to
% calculate the uncertainty.

% MODIS v5 uncertainty given in
% https://link.springer.com/chapter/10.1007%2F978-1-4419-6749-7_24 as
% within 5% with good quality and within 10% even with low-quality data
% (search for "accuracy"). This link
% (https://landval.gsfc.nasa.gov/ProductStatus.php?ProductID=MOD43)
% reiterates the 5%/10% uncertainty and provides a list of references.
% Since we use the BRDF directly, rather than including an RTM, we should
% also include uncertainty from that. Using
% misc_alb_plots.misc_alb_plots.ler_vs_simple_brdf, the 75th percentile
% difference is ~14%, and the slope of LER vs. BRDF doesn't vary much by
% month, surprisingly, so we'll combine these in quadrature to get ( 10^2 +
% 14^2 )^0.5 = 17% uncertainty.

% For terrain pressure, I doubt that there's much uncertainty in GLOBE
% data, and since it is so much smaller than the OMI pixel, I find it
% unlikely that representativeness is an issue. Rather it seems more likely
% to me that whatever error there is in it comes from the fact we assume a
% fixed scale height in our calculation. The best way I've come up with to
% assess that is to compare the average WRF surface pressures to the GLOBE
% surface pressures. Using
% misc_behr_v3_validation.plot_behr_wrf_surfpres_diffs, I get at most a
% -1.5% bias (Jan, Sept, Nov 2012). I will carry that over into the 
% new terrain height.

% For cloud pressure and cloud fraction, Acarreta et al.
% (doi:10.1029/2003JD003915) describes the error in sect. 6.1. They point
% out that it varies with a lot of parameters, and unfortunately the error
% they show in Fig. 3 is not a constant percentage. So we will extract the
% data from Fig. 3 and use it to calculate the error directly given the
% cloud fraction and pressure.
O2 = O2O2CloudUncert();
param_percent_changes = struct('MODISAlbedo', {{17, -17}},...
    'GLOBETerrainHeight', {{@(Data) percent_change_in_range(Data, 'GLOBETerrainHeight', 1.5, [0 Inf]), @(Data) percent_change_in_range(Data, 'GLOBETerrainHeight', -1.5, [0 Inf])}},...
    'BEHRTropopausePressure', {{@(Data) Data.TropopausePressure}},...
    'CloudPressure', {{@(Data) Data.CloudPressure + O2.interpolant(Data.CloudFraction, Data.CloudPressure), @(Data) Data.CloudPressure - O2.interpolant(Data.CloudFraction, Data.CloudPressure)}},...
    'CloudRadianceFraction', {{@(Data) differential_crf(Data, 0.05), @(Data) differential_crf(Data, -0.05)}},...
    'ProfileLoc', 0,...
    'ProfileTime', 0);

% Vary each parameter by the given amount, as well as randomizing the
% profile times and locations. Save the resulting files in subdirectories
% under the output root directory titles by what was varied.
params = fieldnames(param_percent_changes);

for i_param = 1:numel(params)
    this_param = params{i_param};
    output_dir = fullfile(output_root, this_param);
    if ~exist(output_dir, 'dir')
        mkdir(output_dir);
    end
    for i_month = 1:numel(test_months)
        fprintf('Perturbing %s for month %d\n', this_param, test_months(i_month));
        start_date = datenum(test_year, test_months(i_month), 1);
        end_date = datenum(test_year, test_months(i_month), 31);
        parfor this_date = start_date:end_date
            savename = behr_filename(this_date, prof_mode, region);
            savename = strrep(savename, 'BEHR', sprintf('BEHR-%s-UNCERTAINTY', this_param));
            full_savename = fullfile(output_dir, savename);
            if ~overwrite && exist(full_savename, 'file')
                fprintf('%s exists; skipping\n', full_savename);
                continue
            end
            
            this_percent_change = param_percent_changes.(this_param);
            ErrorData = struct('parameter', this_param, 'percent_change_op', this_percent_change, 'Delta', cell(size(this_percent_change)), 'DeltaGrid', cell(size(this_percent_change)));
            
            file_name = behr_filename(this_date, prof_mode, region);
            F = load(fullfile(behr_paths.BEHRMatSubdir(region, prof_mode), file_name));
            Data = F.Data;
            OMI = F.OMI;
            for i_change = 1:numel(this_percent_change)
                [ErrorData(i_change).Delta, ErrorData(i_change).DeltaGrid] = behr_uncertainty_estimation(Data, OMI, params{i_param}, ErrorData(i_change).percent_change_op, 'remove_unchanged_fields', true);
            end
            
            saveoutput(full_savename, ErrorData)
        end
    end
end

end

function saveoutput(savename, ErrorData) %#ok<INUSD>
save(savename, 'ErrorData');
end

function output_root = set_output_root(given_dir, region)
% If no output root directory given, default to a subdirectory titled
% "<region>-uncertainty" in the behr_mat_dir defined by behr_paths.
% Otherwise, require that the given directory exist.
E = JLLErrors;

if isempty(given_dir)
    if ~exist(behr_paths.behr_mat_dir, 'dir')
        E.dir_dne('behr_paths.behr_mat_dir')
    end
    output_root = fullfile(behr_paths.behr_mat_dir, sprintf('%s-uncertainty', lower(region)));
    if ~exist(output_root, 'dir')
        mkdir(output_root)
    end
else
    output_root = given_dir;
    if ~exist(output_root, 'dir')
        E.badinput('Given output root directory (%s) does not exist', given_dir);
    end
end
end

function cldradfrac = differential_crf(Data, cloudfrac_error)
% First, bin the data by 0.05 cloud fraction increments. Any missing bins,
% just remove, will be handled by the interpolation later. Put the bin
% centers such that then fall on 0 and 1.
bin_edges = -0.025:0.05:1.025;
cf_bins = bin_data(Data.CloudFraction, Data.CloudFraction, bin_edges);
crf_bins = bin_data(Data.CloudFraction, Data.CloudRadianceFraction, bin_edges);

cf_means = cellfun(@nanmean, cf_bins);
crf_means = cellfun(@nanmean, crf_bins);
xx = ~isnan(cf_means) & ~isnan(crf_means);
cf_means = cf_means(xx);
crf_means = crf_means(xx);

% Now find each starting cloud radiance fraction on the curve, move the
% cloud fraction over by the error, and take the difference in the cloud
% radiance fractions to get how much the CRF should increase.
crf_diff = interp1(cf_means, crf_means, clipmat(Data.CloudFraction + cloudfrac_error, 0, 1), 'linear', 'extrap')...
    - interp1(cf_means, crf_means, Data.CloudFraction, 'linear', 'extrap');
cldradfrac = clipmat(Data.CloudRadianceFraction + crf_diff, 0, 1);
end

function val = percent_change_in_range(Data, field, percent_change, range)
val = Data.(field);
val = val + val * percent_change / 100;
val = clipmat(val, range);
end
